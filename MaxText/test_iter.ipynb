{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 07:40:03.540359: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 07:40:03.679689: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 07:40:03.680962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 07:40:05.109438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ml_collections\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import tokenizer\n",
    "import sequence_packing\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadjson_and_rekey(ds):\n",
    "    \"\"\"normalization with key mapping\"\"\"\n",
    "    json_specs = {\n",
    "        \"text\": tf.TensorSpec(tf.TensorShape([]), tf.string, name=\"text\"),\n",
    "    }\n",
    "    key_map={\"inputs\": None, \"targets\": \"text\"}\n",
    "    text_max_len = 10 * 1024 * 1024 # 10M\n",
    "\n",
    "    def _loadjson_and_rekey(x, json_specs, key_map=None):\n",
    "        \"\"\"Replace the feature keys according to the mapping in `key_map`.\n",
    "        For example, if the dataset returns examples of the format:\n",
    "        {'foo': 'something', 'bar': 'something else', 'zoo': 'others'}\n",
    "        and key_map = {'boo': 'foo', 'spar': 'bar', 'zoo': None} then this function will return\n",
    "        examples with the format\n",
    "        {'boo': 'something', 'spar': 'something else'}\n",
    "        If a mapping is to None, then the key will be dropped.\n",
    "        Args:\n",
    "          x: an example to process.\n",
    "          key_map: dictionary mapping new keys to original keys\n",
    "        Returns:\n",
    "          A preprocessed example with the format listed above.\n",
    "        \"\"\"\n",
    "        x = tfio.experimental.serialization.decode_json(x, specs=json_specs)\n",
    "        x[\"text\"] = tf.strings.substr(x[\"text\"], 0, text_max_len, unit='BYTE')\n",
    "\n",
    "        x = {\n",
    "            new_key: x[old_key] for new_key, old_key in key_map.items() if old_key\n",
    "        }\n",
    "\n",
    "        return x\n",
    "\n",
    "    return ds.map(\n",
    "        functools.partial(_loadjson_and_rekey, json_specs=json_specs, key_map=key_map), num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "\n",
    "def reduce_concat_tokens(\n",
    "    dataset,\n",
    "    feature_key=\"targets\",\n",
    "    batch_size=128,\n",
    "):\n",
    "    \"\"\"Token-preprocessor to concatenate multiple unrelated documents.\n",
    "    If we want to generate examples of exactly the right length,\n",
    "    (to avoid wasting space on padding), then we use this function, folowed by\n",
    "    split_tokens.\n",
    "    Args:\n",
    "      dataset: a tf.data.Dataset with dictionaries containing the key feature_key.\n",
    "      feature_key: an string\n",
    "      batch_size: an integer - how many documents to concatenate into one\n",
    "    Returns:\n",
    "      a dataset\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {feature_key: x[feature_key]}, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes={feature_key: [-1]})\n",
    "\n",
    "    def _my_fn(x):\n",
    "        tokens = tf.reshape(x[feature_key], [-1])\n",
    "        # strip padding\n",
    "        tokens = tf.boolean_mask(tokens, tf.cast(tokens, tf.bool))\n",
    "        return {feature_key: tokens}\n",
    "\n",
    "    return dataset.map(_my_fn, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "\n",
    "def split_tokens(\n",
    "    dataset,\n",
    "    max_tokens_per_segment=128,\n",
    "    feature_key=\"targets\",\n",
    "):\n",
    "    \"\"\"Split examples into multiple examples each.\n",
    "    The intended use case is to break up long examples for use in unsupervised\n",
    "    transfer-learning.\n",
    "    This function is generally preceded by select_random_chunk.\n",
    "    Args:\n",
    "      dataset: a tf.data.Dataset with dictionaries containing the key feature_key.\n",
    "      max_tokens_per_segment: an integer, the maximum number of tokens in each\n",
    "        segment. Only the final segment may be shorter.\n",
    "      feature_key: a string, the feature to split\n",
    "    Returns:\n",
    "      a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def _split_tokens(x):\n",
    "        \"\"\"Split one token sequence into multiple multiple.\"\"\"\n",
    "        tokens = x[feature_key]\n",
    "        n_tokens = tf.size(tokens)\n",
    "        length = max_tokens_per_segment\n",
    "\n",
    "        # Pad to a multiple of length, then use tf.reshape to split up the tokens\n",
    "        # into num_segments segments each of the given length.\n",
    "        num_segments = tf.cast(\n",
    "            tf.math.ceil(tf.cast(n_tokens, tf.float32) / tf.cast(length, tf.float32)),\n",
    "            tf.int32,\n",
    "        )\n",
    "        padding = num_segments * length - tf.size(tokens)\n",
    "        tokens = tf.pad(tokens, [[0, padding]])\n",
    "        return tf.reshape(tokens, [-1, length])\n",
    "\n",
    "    def _strip_padding(x):\n",
    "        return {feature_key: tf.boolean_mask(x, tf.cast(x, tf.bool))}\n",
    "\n",
    "    # Filter empty examples.\n",
    "    dataset = dataset.filter(lambda x: tf.not_equal(tf.size(x[feature_key]), 0))\n",
    "    dataset = dataset.map(_split_tokens, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.unbatch()\n",
    "    return dataset.map(_strip_padding, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "\n",
    "def split_tokens_to_targets_length(dataset, sequence_length):\n",
    "    return split_tokens(dataset, max_tokens_per_segment=sequence_length)\n",
    "\n",
    "\n",
    "def load_base_dataset(\n",
    "    pattern,\n",
    "    seed,\n",
    "):\n",
    "    data_paths = sorted(tf.io.gfile.glob(pattern))\n",
    "\n",
    "    # shard dataset now\n",
    "    print((pattern, \"all_file_count\", len(data_paths)))\n",
    "    data_num_shards = 1\n",
    "    data_index = 0\n",
    "    data_paths = [\n",
    "        d\n",
    "        for i, d in enumerate(data_paths)\n",
    "        if i % data_num_shards == data_index\n",
    "    ]\n",
    "    random.seed(seed + data_index)\n",
    "    random.shuffle(data_paths)\n",
    "\n",
    "    print((pattern, \"share_file_count\", data_num_shards, data_index, len(data_paths), data_paths[:2]))\n",
    "\n",
    "    ds = tf.data.TextLineDataset(\n",
    "        data_paths,\n",
    "        compression_type=\"GZIP\",\n",
    "        buffer_size=8 * 1024 * 1024,\n",
    "        num_parallel_reads=2,\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/genggui001/gdrive/gg-nlp-lm-new-3\"\n",
    "data_shuffle_seed = 1234\n",
    "max_target_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_en/**/*.jsonl.gz', 'all_file_count', 2088)\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_en/**/*.jsonl.gz', 'share_file_count', 1, 0, 2088, ['/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_en/WebText-en/chunk-00029.jsonl.gz', '/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_en/falcon-refinedweb/chunk-00497.jsonl.gz'])\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_zh/**/*.jsonl.gz', 'all_file_count', 873)\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_zh/**/*.jsonl.gz', 'share_file_count', 1, 0, 873, ['/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_zh/TeleChat-PTD/chunk-00292.jsonl.gz', '/home/genggui001/gdrive/gg-nlp-lm-new-3/gg_zh/WebText-cn/chunk-00153.jsonl.gz'])\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/uonlp_culturax_shuffle/**/*.jsonl.gz', 'all_file_count', 5000)\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/uonlp_culturax_shuffle/**/*.jsonl.gz', 'share_file_count', 1, 0, 5000, ['/home/genggui001/gdrive/gg-nlp-lm-new-3/uonlp_culturax_shuffle/s/schunk-02004.jsonl.gz', '/home/genggui001/gdrive/gg-nlp-lm-new-3/uonlp_culturax_shuffle/s/schunk-00879.jsonl.gz'])\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/the-stack-dedup/**/*.jsonl.gz', 'all_file_count', 200)\n",
      "('/home/genggui001/gdrive/gg-nlp-lm-new-3/the-stack-dedup/**/*.jsonl.gz', 'share_file_count', 1, 0, 200, ['/home/genggui001/gdrive/gg-nlp-lm-new-3/the-stack-dedup/shuffle_data/schunk-00185.jsonl.gz', '/home/genggui001/gdrive/gg-nlp-lm-new-3/the-stack-dedup/shuffle_data/schunk-00132.jsonl.gz'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 07:40:32.292905: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 AVX512F FMA\n",
      "2024-04-25 07:40:32.297564: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load and return dataset of batched examples for use during training.\"\"\"\n",
    "en_ds = load_base_dataset(\n",
    "    pattern=os.path.join(dataset_path, \"gg_en/**/*.jsonl.gz\"),\n",
    "    seed=data_shuffle_seed,\n",
    ")\n",
    "\n",
    "zh_ds = load_base_dataset(\n",
    "    pattern=os.path.join(dataset_path, \"gg_zh/**/*.jsonl.gz\"),\n",
    "    seed=data_shuffle_seed,\n",
    ")\n",
    "\n",
    "other_ds = load_base_dataset(\n",
    "    pattern=os.path.join(dataset_path, \"uonlp_culturax_shuffle/**/*.jsonl.gz\"),\n",
    "    seed=data_shuffle_seed,\n",
    ")\n",
    "\n",
    "code_ds = load_base_dataset(\n",
    "    pattern=os.path.join(dataset_path, \"the-stack-dedup/**/*.jsonl.gz\"),\n",
    "    seed=data_shuffle_seed,\n",
    ")\n",
    "\n",
    "train_ds = tf.data.Dataset.sample_from_datasets(\n",
    "    datasets = [\n",
    "        en_ds.repeat(),\n",
    "        zh_ds.repeat(),\n",
    "        other_ds.repeat(),\n",
    "        code_ds.repeat(),\n",
    "    ], \n",
    "    weights=[\n",
    "        0.45,\n",
    "        0.2,\n",
    "        0.25,\n",
    "        0.1,\n",
    "    ],\n",
    "    seed=data_shuffle_seed,\n",
    ")\n",
    "\n",
    "train_ds = loadjson_and_rekey(\n",
    "    train_ds, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenize_processor = SentencePieceProcessor()\n",
    "tokenize_processor.Load(\"/home/genggui001/code/maxtext/assets/llama_add_world.model\")\n",
    "\n",
    "\n",
    "def tokenize_fn(t: np.ndarray):\n",
    "    text = t.decode('utf-8')\n",
    "    token_ids = tokenize_processor.EncodeAsIds(text)\n",
    "    token_ids = [tokenize_processor.bos_id()] + token_ids + [tokenize_processor.eos_id()]\n",
    "\n",
    "    return np.asarray(token_ids, dtype=np.int32)\n",
    "\n",
    "\n",
    "train_ds = train_ds.shuffle(128, seed=data_shuffle_seed)\n",
    "train_ds = train_ds.map(\n",
    "    lambda x: {\n",
    "        'targets': tf.numpy_function(\n",
    "            func=tokenize_fn, \n",
    "            inp=[x['targets']], \n",
    "            Tout=tf.int32, \n",
    "            stateful=False,\n",
    "        )\n",
    "    },\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    ")\n",
    "\n",
    "train_ds = reduce_concat_tokens(train_ds, feature_key=\"targets\", batch_size=512)\n",
    "train_ds = split_tokens_to_targets_length(train_ds, max_target_length+1)\n",
    "\n",
    "# # note eval_ds is pre tokenized, reduce_concated and splitted to target_length\n",
    "# #   mainly to avoid eval sequences change depending on the number of hosts\n",
    "# train_ds = sequence_packing.pack_dataset(train_ds, max_target_length+1)\n",
    "\n",
    "\n",
    "def format_fn(x):\n",
    "    tokens = x[\"targets\"]\n",
    "    x[\"inputs\"] = tokens[:-1]\n",
    "    x[\"targets\"] = tokens[1:]\n",
    "    \n",
    "    x[\"inputs_segmentation\"] = tf.ones_like(x[\"inputs\"])\n",
    "    x[\"targets_segmentation\"] = x[\"inputs_segmentation\"]\n",
    "\n",
    "    position = tf.range(tf.size(tokens)-1, dtype=tf.int32)\n",
    "\n",
    "    x[\"inputs_position\"] = position\n",
    "    x[\"targets_position\"] = position\n",
    "\n",
    "    return x\n",
    "\n",
    "train_ds = train_ds.map(format_fn, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = train_ds.padded_batch(\n",
    "    8, \n",
    "    padded_shapes={\n",
    "        \"inputs\": max_target_length,\n",
    "        \"targets\": max_target_length,\n",
    "        \"inputs_segmentation\": max_target_length,\n",
    "        \"targets_segmentation\": max_target_length,\n",
    "        \"inputs_position\": max_target_length,\n",
    "        \"targets_position\": max_target_length,\n",
    "    },\n",
    "    drop_remainder=True\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = train_ds.as_numpy_iterator()\n",
    "step = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=step, iterator=iterator)\n",
    "manager = tf.train.CheckpointManager(ckpt, '/home/genggui001/code/maxtext/tmp/tf_ckpts', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f27a119bca0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.restore(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'manager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241m.\u001b[39mlatest_checkpoint\n",
      "\u001b[0;31mNameError\u001b[0m: name 'manager' is not defined"
     ]
    }
   ],
   "source": [
    "manager.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=64>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genggui001/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]2024-04-25 07:24:01.904674: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 3 of 128\n",
      "2024-04-25 07:24:02.787369: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 4 of 128\n",
      "2024-04-25 07:24:02.787420: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 5 of 128\n",
      "2024-04-25 07:24:03.355352: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 12 of 128\n",
      "2024-04-25 07:24:03.464820: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n",
      "100%|██████████| 64/64 [01:00<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(64)):\n",
    "    tmp = next(iterator)\n",
    "    step.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/genggui001/code/maxtext/tmp/tf_ckpts/ckpt-64'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.save(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint_management.CheckpointManager at 0x7fecec765240>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'targets': array([[  527, 29872, 29946, ...,   293,  6959,  7582],\n",
       "        [ 2966, 29872, 29946, ...,  4468,  5173,   334],\n",
       "        [  317,  1303,   736, ..., 31038, 30745, 33118],\n",
       "        ...,\n",
       "        [29946, 29893, 29872, ..., 30215, 32176, 36023],\n",
       "        [30504, 33955, 30215, ...,  1493, 12232,  2134],\n",
       "        [   14, 14805,  2306, ..., 30331, 37771, 35690]], dtype=int32),\n",
       " 'inputs': array([[29880,   527, 29872, ...,   568,   293,  6959],\n",
       "        [  324,  2966, 29872, ..., 60983,  4468,  5173],\n",
       "        [13806,   317,  1303, ..., 52506, 31038, 30745],\n",
       "        ...,\n",
       "        [29956, 29946, 29893, ..., 34880, 30215, 32176],\n",
       "        [32570, 30504, 33955, ...,   279,  1493, 12232],\n",
       "        [29890,    14, 14805, ..., 40356, 30331, 37771]], dtype=int32),\n",
       " 'inputs_segmentation': array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]], dtype=int32),\n",
       " 'targets_segmentation': array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]], dtype=int32),\n",
       " 'inputs_position': array([[   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        ...,\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095]], dtype=int32),\n",
       " 'targets_position': array([[   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        ...,\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095],\n",
       "        [   0,    1,    2, ..., 4093, 4094, 4095]], dtype=int32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
